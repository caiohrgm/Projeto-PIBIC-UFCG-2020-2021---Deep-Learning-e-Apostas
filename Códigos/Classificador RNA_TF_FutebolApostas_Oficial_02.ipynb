{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classificador RNA_TF_FutebolApostas_Oficial_02.ipynb","provenance":[{"file_id":"1F8tDvTKQ8ksrjDFUqmdkmMDiiQuzINNy","timestamp":1623871853786},{"file_id":"1wA-R8pan4CE9OLvoA4Dgv6rn3aPExA_V","timestamp":1623434119903},{"file_id":"1LwXxw0968W8XzAB8aoACzBUlQOeuNelf","timestamp":1623269283332},{"file_id":"1K8XeTYx-I420g1CrHkF7A7lBTHv58KsN","timestamp":1618597003614},{"file_id":"1_qseKgbE1Ib2EPBINlSWdKRe6fmQYSB3","timestamp":1616667268706},{"file_id":"1xK7AkdGFCH8AtXvvlWd8yLnaDnTbHQK0","timestamp":1614544775330}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qt213sAXa9ep"},"source":["**IMPORTS**"]},{"cell_type":"code","metadata":{"id":"lZVYRs1wyWKm"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from google.colab import files\n","from matplotlib import pyplot\n","from keras.callbacks import LearningRateScheduler\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JFImE6wW9Ra"},"source":[" **1. FUNÇÕES**"]},{"cell_type":"code","metadata":{"id":"0YLof0DaW8X_"},"source":["'''Funções Estatísticas'''\n","def acuracia(tp,tn,total):\n","  return np.round((tp+tn)/total,4)\n","def precisao(tp,fp):\n","  return np.round(tp/(tp+fp),4)\n","def sensibilidade(tp,fn): #Sensibilidade\n","  return np.round(tp/(tp+fn),4)\n","def f1_score(tp,fp,fn):\n","  p = precisao(tp,fp)\n","  r = sensibilidade(tp,fn)\n","  return np.round((2 * p * r) / (p + r),4)\n","def step_decay_schedule(initial_lr, decay_factor, step_size):\n","    def schedule(epoch):\n","        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n","    return LearningRateScheduler(schedule)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lO3vCM2syWOs","executionInfo":{"status":"ok","timestamp":1627311383573,"user_tz":180,"elapsed":258,"user":{"displayName":"Alexsandro B. Cavalcanti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEVD2n0m6-vqBZN7lk3LblR32LRVVIjABJgcNJ=s64","userId":"09679722210023791345"}}},"source":["'''Função de Carregamento dos arquivos'''\n","def load_data(tables_path):\n","  lista_temp = os.listdir(tables_path) \n","  lista_temp.sort() # Ordena a lista com os nomes dos arquivos;\n","  lista_testes = []\n","  for i in range(10):\n","    temp = str(lista_temp[i])\n","    # Abrindo o arquivo da temporada e convertendo em DataFrame:\n","    datasetTemporada = pd.read_csv(tables_path+temp)\n","    df = pd.DataFrame(datasetTemporada)\n","    #Removendo coluna 'Time'(Tempo, em Inglês): (Atualizado em: 24_02_2021)\n","    lista_colunas = df.columns.values.tolist()\n","    if lista_colunas[2] == 'Time':\n","        df = df.drop(['Time'],axis=1)\n","    #Corrigindo diferença de quantidade de colunas: (Atualizado em: 24_02_2021)\n","    columns_size = df.shape[1]\n","    if columns_size < 105 and columns_size > 26:\n","        diff = 105 - columns_size\n","        j = 0 \n","        name_column = ''\n","        while j < diff:\n","            df[name_column] = \"\"\n","            name_column = name_column+\"-\"\n","            j = j + 1\n","    # Filtra o DataFrame original com apenas as colunas necessárias para análise;\n","    odds = np.arange(26,105) # atribuindo o intervalo das colunas 25 a 70, onde estão as odds não analisadas. (retorna valores espaçados igualmente dentro de um intervalo definido)\n","    df_filtrado = df.drop(df.columns[odds], axis = 1, inplace = True)  # Removendo as colunas de odds (permanentemente do DF original)\n","    df_filtrado = df.drop(columns = ['Div','Date','HTHG','HTAG','HTR','HF','AF','HC',\n","                                'AC','HY','AY','HR','AR','Referee','B365D','B365A']) # Adicionei à remoção: B365D,B365A e as colunas dos times(Atualizado em: 25_02_2021)\n","    if len(df_filtrado) > 380:  # Corrigindo uma linha fantasma que surge no df, quando importa por google drive!\n","        df_filtrado = df_filtrado[:-1]\n","    # Adicionando a coluna oddLayH no DF \"menor\";\n","    oddsLayH = []\n","    for k in range(380):\n","        oddH = float(df_filtrado.iloc[k][9])\n","        oddLayH = 1/(1-(1/oddH))\n","        oddsLayH.append(np.round(oddLayH,2))\n","    df_filtrado['oddLayH'] = oddsLayH\n","    # Convertendo resultados de FTR para binario(Vitoria=1, Derrota=0 ,Empate=0) no DF \"menor\";\n","    listR = []\n","    for l in range(380): \n","        if df_filtrado.iloc[l,4] == 'H':\n","            listR.append(1)\n","        else:\n","            listR.append(0)\n","    df_filtrado['FTR'] = listR\n","    # Reposicionando as colunas:\n","    df_filtrado = df_filtrado[['HomeTeam','AwayTeam','FTHG','FTAG','HS','AS','HST','AST','FTR','B365H','oddLayH']]\n","    # Renomeando colunas B365H:\n","    df_filtrado.rename(columns = {'B365H':'oddH','HomeTeam':'Mandante','AwayTeam':'Visitante','FTHG':'GPM','FTAG':'GPV','HS':'CM','AS':'CV','HST':'CAAM','AST':'CAAV','FTR':'Resultado'}, inplace = True)\n","    lista_testes.append(df_filtrado)\n","  return lista_testes"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"0w4VAWDfLYOV"},"source":["'''Função que gera novos dataframes com as médias de cada temporada, calculadas rodada a rodada,excluindo-se as 4 rodadas iniciais:'''\n","def gera_df_medias_temporada(df_temporada):\n","  temp = df_temporada\n","  rodada = 5\n","  novos_dfs_rodadas = []\n","  df_temp = pd.DataFrame()\n","  while rodada <= 38:\n","    #pula as X linhas iniciais,para gerar médias:\n","    rodada_teste = rodada\n","    max = (rodada_teste*10) - 10\n","    df = temp.iloc[0:max,0:8] # Gera um DataFrame com todas as linhas antes da rodada atual. A media sera calcula apenas com os valores anterior a rodada.\n","    # Coleta os times do campeonato:\n","    times = {}\n","    for i in range(40):\n","      time = df.iloc[i][0]\n","      if time in times:\n","        pass\n","      else:\n","        times[time] = []\n","    # Colentado as médias de cada time, para a rodada especificada:\n","    for nome_do_time in times.keys():\n","      '''Mandante'''\n","      temp_df = df.query('Mandante==\"'+nome_do_time+'\"')\n","      media_GPM = np.round(temp_df['GPM'].mean(),2)\n","      media_CM = np.round(temp_df['CM'].mean(),2)\n","      media_CAAM = np.round(temp_df['CAAM'].mean(),2)\n","      '''Visitante'''\n","      temp_df = df.query('Visitante==\"'+nome_do_time+'\"')\n","      media_GPV = np.round(temp_df['GPV'].mean(),2)\n","      media_CV = np.round(temp_df['CV'].mean(),2)\n","      media_CAAV = np.round(temp_df['CAAV'].mean(),2)\n","\n","      times[nome_do_time].append(media_GPM)\n","      times[nome_do_time].append(media_GPV)\n","      times[nome_do_time].append(media_CM)\n","      times[nome_do_time].append(media_CV)\n","      times[nome_do_time].append(media_CAAM)\n","      times[nome_do_time].append(media_CAAV)\n","    # Gerando novo dataFrame com as medias da rodada específica:\n","    min = (rodada*10)-10\n","    max = min + 9\n","    df = temp\n","    df_copy = pd.DataFrame.copy(df,deep=True)  # Deep: se for True, as alterações feitas na cópia não transferem para o original;\n","    df_copy['GPM'] = df_copy['GPM'].astype(float)\n","    df_copy['GPV'] = df_copy['GPV'].astype(float)\n","    df_copy['CM'] = df_copy['CM'].astype(float)\n","    df_copy['CV'] = df_copy['CV'].astype(float)\n","    df_copy['CAAM'] = df_copy['CAAM'].astype(float)\n","    df_copy['CAAV'] = df_copy['CAAV'].astype(float)\n","    # Substituindo o dataframe original pelas medias de cada atributo:\n","    dic = times\n","    for time in times.keys():\n","      reqd_Index = df[df['Mandante']==time].index.tolist() # Encontra todos os indices onde o time é Mandante;\n","      for i in reqd_Index:\n","        if i >= min and i < min+10:\n","          df_copy.iat[i,2] = np.round(dic[time][0],2)\n","          df_copy.iat[i,4] = np.round(dic[time][2],2)\n","          df_copy.iat[i,6] = np.round(dic[time][4],2)\n","      reqd_Index = df[df['Visitante']==time].index.tolist() # Encontra todos os indices onde o time é Visitante;\n","      for i in reqd_Index:\n","        if i >= min and i < min+10:\n","          df_copy.iat[i,3] = np.round(dic[time][1],2)\n","          df_copy.iat[i,5] = np.round(dic[time][3],2)\n","          df_copy.iat[i,7] = np.round(dic[time][5],2)\n","    df_copy.rename(columns = {'GPM':'MediaGPM','GPV':'MediaGPV','CM':'MediaCM','CV':'MediaCV','CAAM':'MediaCAAM','CAAV':'MediaCAAV'}, inplace = True)\n","    df_medias_rodada_treino = df_copy[min:max+1]\n","    novos_dfs_rodadas.append(df_medias_rodada_treino)\n","    '''print(Fore.GREEN+Style.BRIGHT+\"RODADA: %d\" % rodada+Style.RESET_ALL)\n","    display(df_medias_rodada)\n","    print('\\n')\n","    print('\\n')'''\n","    rodada += 1\n","  df_medias_temporada = df_temp.append(novos_dfs_rodadas,ignore_index=True)\n","  return df_medias_temporada\n","\n","''' Função que armazena todos os dataframes com médias de cada temporada em uma lista '''\n","def gera_lista_medias_temporadas(dfs):  \n","  # Lista com os dfs das temproadas com as medias, rodada a rodada.\n","  dfs_medias_temporadas = []\n","  for df in dfs:\n","    temp_df = gera_df_medias_temporada(df)\n","    dfs_medias_temporadas.append(temp_df)\n","  return dfs_medias_temporadas\n","\n","'''Função que retorna os dataframes concatenados dos treinos base,armazenados em uma lista'''\n","def gera_lista_temporadas_treino_base(lista_medias_temporadas):\n","  # Concatena temporadas treino base:\n","  lista = lista_medias_temporadas\n","  i = 0\n","  base_01 = 0\n","  base_02 = 1\n","  base_03 = 2\n","  datasets_de_treinos_iniciais = []\n","  while i < 7:\n","    df_01 = lista[base_01]\n","    df_02 = lista[base_02]\n","    df_03 = lista[base_03]\n","    temp_df = pd.concat([df_01,df_02,df_03],ignore_index=True)\n","    datasets_de_treinos_iniciais.append(temp_df)\n","    ##### Iteração:\n","    base_01 += 1\n","    base_02 += 1\n","    base_03 += 1\n","    i += 1\n","  return datasets_de_treinos_iniciais\n","\n","'''Método que retorna um dataframe com médias da rodada a ser preditada'''\n","def gera_novo_df_com_medias_da_rodada_teste(rodada,dataFrame):\n","  min = (rodada - 5) * 10\n","  max = min + 10\n","  return dataFrame.iloc[min:max,0:11]\n","\n","'''Método que junta o dataframe com médias da rodada ao treino com médias'''\n","def concatena_rodada_teste_ao_treino(df_treino,df_medias_rodada_teste):\n","  size = len(df_treino)\n","  temp_df = pd.concat([df_treino,df_medias_rodada_teste], ignore_index=True)\n","  size += 10\n","  temp_df.index = range(size)\n","  return temp_df\n","\n","'''Méotodo que prepara o dataframe para ser inserido como dataset X na Rede Neural; serve para treino e para teste'''\n","def cria_X(dataFrame):\n","  temp_df = dataFrame.drop(columns = ['Mandante', 'Visitante', 'Resultado','oddLayH'])\n","  return temp_df\n","\n","'''Méotodo que prepara o dataframe para ser inserido como dataset Y na Rede neural; serve para treino e para teste'''\n","def cria_Y(dataFrame):\n","  temp_df = dataFrame.drop(columns = ['Mandante', 'Visitante','MediaGPM','MediaGPV','MediaCM','MediaCV','MediaCAAM','MediaCAAV','oddH','oddLayH'])\n","  return temp_df\n","  \n","'''Méotodo que prepara o df da rodada para ser predições, com apenas as odds e o resultado da partida'''\n","def cria_base_predicao_da_rodada(dataFrame):\n","  temp_df = dataFrame.drop(columns = ['Mandante', 'Visitante','MediaGPM','MediaGPV','MediaCM','MediaCV','MediaCAAM','MediaCAAV'])\n","  return temp_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-WOPifgXS4q"},"source":["**2.EXECUÇÃO**"]},{"cell_type":"code","metadata":{"id":"nzSJ3-UoWY7u"},"source":["path = '/content/drive/MyDrive/Iniciação Científica - UAEst - Prof. Alexsandro_2019.2/Pesquisa_Caio Medeiros/Projeto 2020_2021_Deep Learning/Tabelas - Premier League/Tabelas Premier League/'\n","dfs = load_data(path)\n","\n","lista_medias_temporadas = gera_lista_medias_temporadas(dfs) # Lista de todas as temporadas com médias\n","lista_treinos_base = gera_lista_temporadas_treino_base(lista_medias_temporadas) # Lista de todos os treinos base (7 elementos)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Aiye4oTmVcK"},"source":["df = lista_medias_temporadas[0]\n","writer = '/content/drive/MyDrive/Iniciação Científica - UAEst - Prof. Alexsandro_2019.2/Pesquisa_Caio Medeiros/Projeto 2020_2021_Deep Learning/Resultados/lista_geral.xlsx'\n","df.to_excel(writer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJMEKQgr-wQB"},"source":["**ENTRADA DE PARÂMETROS**"]},{"cell_type":"code","metadata":{"id":"4ZpfV3VY-1-r"},"source":["neuronios = 7 # 7 | 8 | 16 | 32 | 64 | 128\n","camadas = 7   # 1 | 2 | 4 | 8 | 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TdyelJe_Emp"},"source":["**EXECUÇÃO DA REDE**"]},{"cell_type":"code","metadata":{"id":"MTOlTIcL6MjO"},"source":["model = tf.keras.models.Sequential()\n","#model.add(tf.keras.layers.Dense(neuronios, input_dim = 7, kernel_initializer = 'uniform', activation = 'relu'))\n","for i in range(camadas):\n","  if i == 0:\n","    model.add(tf.keras.layers.Dense(neuronios, input_dim = 7, kernel_initializer = 'uniform', activation = 'relu'))\n","    if camadas == 1:\n","      break\n","  else:\n","    model.add(tf.keras.layers.Dense(neuronios, activation = 'relu'))\n","\n","\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n","model.compile(optimizer = opt, loss='binary_crossentropy', metrics=['accuracy'])\n","mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose = 0, save_best_only=True)\n","\n","temporada = 2013\n","teste = 3\n","treino_base = 0\n","stats_geral = []\n","analise_exp_temporadas = []\n","while temporada < 2020 and treino_base < 7:\n","\n","  if temporada > 2013:\n","    saved_model = load_model('best_model.h5')\n","    saved_model.compile(optimizer = opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    mc = ModelCheckpoint('best_model.h5', monitor = 'val_loss', mode='min', verbose = 0, save_best_only=True)\n","\n","  temp_df = lista_treinos_base[treino_base] # vai do 0 ao 6 (7 treinos ao total)\n","  rodada = 5\n","  l_p_rodada_intervalos = {}\n","  lp_geral_rodada_intervalos = {}\n","  tp = 0\n","  fp = 0\n","  fn = 0\n","  tn = 0\n","  tp_rodada = 0\n","  fp_rodada = 0\n","  fn_rodada = 0\n","  tn_rodada = 0\n","  \n","  lista_probs = {}\n","\n","  while rodada <= 38:\n","\n","    '''if rodada > 5:\n","      saved_model = load_model('best_model.h5')\n","      saved_model.compile(tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n","      mc = ModelCheckpoint('best_model.h5', monitor = 'val_loss', mode='min', verbose = 0, save_best_only=True)'''\n","    \n","    # Montando os datasets de Treino X e Y (começará a partir dar 4ª round e terá médias em vez de valores base, com as 50 primeira linhas depois das 30 iniciais):\n","    data_x_train = cria_X(temp_df)\n","    data_y_train = cria_Y(temp_df)        \n","    x_train = tf.convert_to_tensor(data_x_train, dtype = tf.int32)  \n","    y_train = tf.convert_to_tensor(data_y_train, dtype = tf.int32)\n","    # Montando os datasets de teste X e Y:\n","    df_rodada_teste = gera_novo_df_com_medias_da_rodada_teste(rodada,lista_medias_temporadas[teste])\n","    data_x_test = cria_X(df_rodada_teste)\n","    data_y_test = cria_Y(df_rodada_teste)\n","    x_test = tf.convert_to_tensor(data_x_test, dtype = tf.float32)  \n","    y_test = tf.convert_to_tensor(data_y_test, dtype = tf.float32)  \n","\n","    # Treinando o modelo sem/com parada antecipada e/ou decaimento:\n","    lr_sched = step_decay_schedule(initial_lr = 0.0001, decay_factor = 0.25, step_size = 10)\n","    \n","    es = EarlyStopping(monitor='val_loss', mode='min', verbose = 0, patience = 17)\n","    model.fit(x_train, y_train, validation_split = 0.2, epochs = 1,callbacks =[es,mc], verbose = 0)  # Analisar os parametros possiveis. Verificar Sobreajuste e Subajuste\n","    # Analisando as predições e o Lucro/prejuízo:\n","    df_base_predicoes = cria_base_predicao_da_rodada(df_rodada_teste)\n","    rna_predicoes = model.predict(x_test)\n","    \n","    lista_probs[str(rodada)] = rna_predicoes\n","\n","    prob_min = 0.5\n","    prob_max = 0.55\n","    ganhos_geral_intervalos = []\n","    perdas_geral_intervalos = []\n","    l_p_geral_intervalos = []\n","\n","    while prob_max < 1.05:\n","\n","      ganhos_intervalo = 0\n","      perdas_intervalo = 0\n","      \n","      for i in range(10):\n","        result = df_base_predicoes.iloc[i][0]\n","        oddH = df_base_predicoes.iloc[i][1]\n","        oddLayH = df_base_predicoes.iloc[i][2]\n","        alg_prob = rna_predicoes[i][0]\n","\n","        if alg_prob > prob_min and alg_prob <= prob_max:\n","          if result == 1:\n","            ganhos_intervalo += 100*(oddH - 1) \n","          else:\n","            perdas_intervalo += -100\n","        elif alg_prob <= prob_min and (1-alg_prob) > prob_min and (1-alg_prob) <= prob_max:\n","          if result == 0:\n","            ganhos_intervalo += 100*(oddLayH - 1)\n","          else:\n","            perdas_intervalo += -100\n","      \n","      \n","      ganhos_geral_intervalos.append(np.round(ganhos_intervalo,2))\n","      perdas_geral_intervalos.append(np.round(perdas_intervalo,2))\n","      l_p_geral_intervalos.append(np.round(ganhos_intervalo+perdas_intervalo,2))\n","\n","      prob_min += 0.05\n","      prob_max += 0.05\n","    \n","    l_p_rodada_intervalos[str(rodada)] = (ganhos_geral_intervalos, perdas_geral_intervalos)\n","    lp_geral_rodada_intervalos[str(rodada)] =  l_p_geral_intervalos\n","    \n","    # Últimas etapas:\n","    temp_df = concatena_rodada_teste_ao_treino(temp_df,df_rodada_teste)\n","    rodada += 1\n","\n","  teste += 1\n","  treino_base += 1\n","  \n","\n","  # Gerando Palnilha de LP geral da temporada:\n","  listona = []\n","  for i in range(5,39):\n","    listona.append(lp_geral_rodada_intervalos[str(i)])\n","\n","  colunas = ['0.5 - 0.55', '0.55 - 0.6', '0.6 - 0.65', '0.65 - 0.7', '0.7 - 0.75', \n","             '0.75 - 0.8', '0.8 - 0.85', '0.85 - 0.9', '0.9 - 0.95', '0.95 - 1.0',]\n","  index_rodadas = []\n","  rodada = 5\n","\n","  '''for i in range(5,39):\n","    index_rodadas.append(rodada)\n","    rodada +=1\n","  rows = listona\n","  df_02 = pd.DataFrame(rows,columns = colunas,index = index_rodadas)\n","  writer = '/content/drive/MyDrive/Iniciação Científica - UAEst - Prof. Alexsandro_2019.2/Pesquisa_Caio Medeiros/Projeto 2020_2021_Deep Learning/Resultados/Análise_LP_probabilidades/resultado_'+str(temporada)+'_Análise de Intervalos de Probabilidade_com_'+str(camadas)+'_camadas_'+str(neuronios)+'_neuronios.xlsx'\n","  df_02.to_excel(writer)'''\n","\n","  # Gerando Palnilha de Lucro e Prejuízos separados da temporada:\n","  listona = []\n","  for i in range(5,39):\n","    lista_maior_temp = l_p_rodada_intervalos[str(i)][0]+l_p_rodada_intervalos[str(i)][1]\n","    listona.append(lista_maior_temp)\n","\n","  colunas = ['0.5 - 0.55L', '0.55 - 0.6L', '0.6 - 0.65L', '0.65 - 0.7L', '0.7 - 0.75L', \n","             '0.75 - 0.8L', '0.8 - 0.85L', '0.85 - 0.9L', '0.9 - 0.95L', '0.95 - 1.0L',\n","             '0.5 - 0.55P', '0.55 - 0.6P', '0.6 - 0.65P', '0.65 - 0.7P', '0.7 - 0.75P', \n","             '0.75 - 0.8P', '0.8 - 0.85P', '0.85 - 0.9P', '0.9 - 0.95P', '0.95 - 1.0P']\n","\n","  index_rodadas = []\n","  rodada = 5\n","  '''for i in range(5,39):\n","    index_rodadas.append(rodada)\n","    rodada +=1\n","  rows = listona\n","  df_02 = pd.DataFrame(rows,columns = colunas,index = index_rodadas)\n","  writer = '/content/drive/MyDrive/Iniciação Científica - UAEst - Prof. Alexsandro_2019.2/Pesquisa_Caio Medeiros/Projeto 2020_2021_Deep Learning/Resultados/Análise_LP_probabilidades/Analise De Lucro e Prejuizo_detalhada/resultado_'+str(temporada)+'_Análise detalhada de Intervalos_com_'+str(camadas)+'_camadas_'+str(neuronios)+'_neuronios.xlsx'\n","  df_02.to_excel(writer)'''\n","\n","  temporada += 1\n"],"execution_count":null,"outputs":[]}]}