{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classificador RNA_TF_FutebolApostas_Oficial_02_Só no Mandante.ipynb","provenance":[{"file_id":"1F8tDvTKQ8ksrjDFUqmdkmMDiiQuzINNy","timestamp":1623871853786},{"file_id":"1wA-R8pan4CE9OLvoA4Dgv6rn3aPExA_V","timestamp":1623434119903},{"file_id":"1LwXxw0968W8XzAB8aoACzBUlQOeuNelf","timestamp":1623269283332},{"file_id":"1K8XeTYx-I420g1CrHkF7A7lBTHv58KsN","timestamp":1618597003614},{"file_id":"1_qseKgbE1Ib2EPBINlSWdKRe6fmQYSB3","timestamp":1616667268706},{"file_id":"1xK7AkdGFCH8AtXvvlWd8yLnaDnTbHQK0","timestamp":1614544775330}],"collapsed_sections":[],"mount_file_id":"1ULPMQAJD5zUkxdQlfwnH61GkZsFcBM_Q","authorship_tag":"ABX9TyMgUbmb5vFGP1yafmuxjzzc"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"MFDsr2jWrclp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624024721056,"user_tz":180,"elapsed":2788,"user":{"displayName":"Caio Medeiros","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQG435jXGXnbmJ1O_I3S2jlpLhSSV6mfsEHit2=s64","userId":"03125895587331329917"}},"outputId":"4b58ceab-525f-411c-f75d-3f19813a9701"},"source":["!pip install colorama"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qt213sAXa9ep"},"source":["**IMPORTS**"]},{"cell_type":"code","metadata":{"id":"lZVYRs1wyWKm","executionInfo":{"status":"ok","timestamp":1624033532044,"user_tz":180,"elapsed":449,"user":{"displayName":"Caio Medeiros","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQG435jXGXnbmJ1O_I3S2jlpLhSSV6mfsEHit2=s64","userId":"03125895587331329917"}}},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from google.colab import files\n","from matplotlib import pyplot\n","from keras.callbacks import LearningRateScheduler\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","# modulo de cor:\n","import colorama\n","from colorama import Fore\n","from colorama import Style"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JFImE6wW9Ra"},"source":[" **1. FUNÇÕES**"]},{"cell_type":"code","metadata":{"id":"0YLof0DaW8X_","executionInfo":{"status":"ok","timestamp":1624033534133,"user_tz":180,"elapsed":2,"user":{"displayName":"Caio Medeiros","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQG435jXGXnbmJ1O_I3S2jlpLhSSV6mfsEHit2=s64","userId":"03125895587331329917"}}},"source":["'''Funções Estatísticas'''\n","def acuracia(tp,tn,total):\n","  return np.round((tp+tn)/total,4)\n","def precisao(tp,fp):\n","  return np.round(tp/(tp+fp),4)\n","def sensibilidade(tp,fn): #Sensibilidade\n","  return np.round(tp/(tp+fn),4)\n","def f1_score(tp,fp,fn):\n","  p = precisao(tp,fp)\n","  r = sensibilidade(tp,fn)\n","  return np.round((2 * p * r) / (p + r),4)\n","def step_decay_schedule(initial_lr, decay_factor, step_size):\n","    def schedule(epoch):\n","        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n","    return LearningRateScheduler(schedule)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"lO3vCM2syWOs","executionInfo":{"status":"ok","timestamp":1624033536416,"user_tz":180,"elapsed":2,"user":{"displayName":"Caio Medeiros","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQG435jXGXnbmJ1O_I3S2jlpLhSSV6mfsEHit2=s64","userId":"03125895587331329917"}}},"source":["'''Função de Carregamento dos arquivos'''\n","def load_data(tables_path):\n","  lista_temp = os.listdir(tables_path) \n","  lista_temp.sort() # Ordena a lista com os nomes dos arquivos;\n","  lista_testes = []\n","  for i in range(10):\n","    temp = str(lista_temp[i])\n","    # Abrindo o arquivo da temporada e convertendo em DataFrame:\n","    datasetTemporada = pd.read_csv(tables_path+temp)\n","    df = pd.DataFrame(datasetTemporada)\n","    #Removendo coluna 'Time'(Tempo, em Inglês): (Atualizado em: 24_02_2021)\n","    lista_colunas = df.columns.values.tolist()\n","    if lista_colunas[2] == 'Time':\n","        df = df.drop(['Time'],axis=1)\n","    #Corrigindo diferença de quantidade de colunas: (Atualizado em: 24_02_2021)\n","    columns_size = df.shape[1]\n","    if columns_size < 105 and columns_size > 26:\n","        diff = 105 - columns_size\n","        j = 0 \n","        name_column = ''\n","        while j < diff:\n","            df[name_column] = \"\"\n","            name_column = name_column+\"-\"\n","            j = j + 1\n","    # Filtra o DataFrame original com apenas as colunas necessárias para análise;\n","    odds = np.arange(26,105) # atribuindo o intervalo das colunas 25 a 70, onde estão as odds não analisadas. (retorna valores espaçados igualmente dentro de um intervalo definido)\n","    df_filtrado = df.drop(df.columns[odds], axis = 1, inplace = True)  # Removendo as colunas de odds (permanentemente do DF original)\n","    df_filtrado = df.drop(columns = ['Div','Date','HTHG','HTAG','HTR','HF','AF','HC',\n","                                'AC','HY','AY','HR','AR','Referee','B365D','B365A']) # Adicionei à remoção: B365D,B365A e as colunas dos times(Atualizado em: 25_02_2021)\n","    if len(df_filtrado) > 380:  # Corrigindo uma linha fantasma que surge no df, quando importa por google drive!\n","        df_filtrado = df_filtrado[:-1]\n","    # Adicionando a coluna oddLayH no DF \"menor\";\n","    oddsLayH = []\n","    for k in range(380):\n","        oddH = float(df_filtrado.iloc[k][9])\n","        oddLayH = 1/(1-(1/oddH))\n","        oddsLayH.append(np.round(oddLayH,2))\n","    df_filtrado['oddLayH'] = oddsLayH\n","    # Convertendo resultados de FTR para binario(Vitoria=1, Derrota=0 ,Empate=0) no DF \"menor\";\n","    listR = []\n","    for l in range(380): \n","        if df_filtrado.iloc[l,4] == 'H':\n","            listR.append(1)\n","        else:\n","            listR.append(0)\n","    df_filtrado['FTR'] = listR\n","    # Reposicionando as colunas:\n","    df_filtrado = df_filtrado[['HomeTeam','AwayTeam','FTHG','FTAG','HS','AS','HST','AST','FTR','B365H','oddLayH']]\n","    # Renomeando colunas B365H:\n","    df_filtrado.rename(columns = {'B365H':'oddH','HomeTeam':'Mandante','AwayTeam':'Visitante','FTHG':'GPM','FTAG':'GPV','HS':'CM','AS':'CV','HST':'CAAM','AST':'CAAV','FTR':'Resultado'}, inplace = True)\n","    lista_testes.append(df_filtrado)\n","  return lista_testes"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"0w4VAWDfLYOV","executionInfo":{"status":"ok","timestamp":1624033539032,"user_tz":180,"elapsed":875,"user":{"displayName":"Caio Medeiros","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQG435jXGXnbmJ1O_I3S2jlpLhSSV6mfsEHit2=s64","userId":"03125895587331329917"}}},"source":["'''Função que gera novos dataframes com as médias de cada temporada, calculadas rodada a rodada,excluindo-se as 4 rodadas iniciais:'''\n","def gera_df_medias_temporada(df_temporada):\n","  temp = df_temporada\n","  rodada = 5\n","  novos_dfs_rodadas = []\n","  df_temp = pd.DataFrame()\n","  while rodada <= 38:\n","    #pula as X linhas iniciais,para gerar médias:\n","    rodada_teste = rodada\n","    max = (rodada_teste*10) - 10\n","    df = temp.iloc[0:max,0:8] # Gera um DataFrame com todas as linhas antes da rodada atual. A media sera calcula apenas com os valores anterior a rodada.\n","    # Coleta os times do campeonato:\n","    times = {}\n","    for i in range(40):\n","      time = df.iloc[i][0]\n","      if time in times:\n","        pass\n","      else:\n","        times[time] = []\n","    # Colentado as médias de cada time, para a rodada especificada:\n","    for nome_do_time in times.keys():\n","      '''Mandante'''\n","      temp_df = df.query('Mandante==\"'+nome_do_time+'\"')\n","      media_GPM = np.round(temp_df['GPM'].mean(),2)\n","      media_CM = np.round(temp_df['CM'].mean(),2)\n","      media_CAAM = np.round(temp_df['CAAM'].mean(),2)\n","      '''Visitante'''\n","      temp_df = df.query('Visitante==\"'+nome_do_time+'\"')\n","      media_GPV = np.round(temp_df['GPV'].mean(),2)\n","      media_CV = np.round(temp_df['CV'].mean(),2)\n","      media_CAAV = np.round(temp_df['CAAV'].mean(),2)\n","\n","      times[nome_do_time].append(media_GPM)\n","      times[nome_do_time].append(media_GPV)\n","      times[nome_do_time].append(media_CM)\n","      times[nome_do_time].append(media_CV)\n","      times[nome_do_time].append(media_CAAM)\n","      times[nome_do_time].append(media_CAAV)\n","    # Gerando novo dataFrame com as medias da rodada específica:\n","    min = (rodada*10)-10\n","    max = min + 9\n","    df = temp\n","    df_copy = pd.DataFrame.copy(df,deep=True)  # Deep: se for True, as alterações feitas na cópia não transferem para o original;\n","    df_copy['GPM'] = df_copy['GPM'].astype(float)\n","    df_copy['GPV'] = df_copy['GPV'].astype(float)\n","    df_copy['CM'] = df_copy['CM'].astype(float)\n","    df_copy['CV'] = df_copy['CV'].astype(float)\n","    df_copy['CAAM'] = df_copy['CAAM'].astype(float)\n","    df_copy['CAAV'] = df_copy['CAAV'].astype(float)\n","    # Substituindo o dataframe original pelas medias de cada atributo:\n","    dic = times\n","    for time in times.keys():\n","      reqd_Index = df[df['Mandante']==time].index.tolist() # Encontra todos os indices onde o time é Mandante;\n","      for i in reqd_Index:\n","        if i >= min and i < min+10:\n","          df_copy.iat[i,2] = np.round(dic[time][0],2)\n","          df_copy.iat[i,4] = np.round(dic[time][2],2)\n","          df_copy.iat[i,6] = np.round(dic[time][4],2)\n","      reqd_Index = df[df['Visitante']==time].index.tolist() # Encontra todos os indices onde o time é Visitante;\n","      for i in reqd_Index:\n","        if i >= min and i < min+10:\n","          df_copy.iat[i,3] = np.round(dic[time][1],2)\n","          df_copy.iat[i,5] = np.round(dic[time][3],2)\n","          df_copy.iat[i,7] = np.round(dic[time][5],2)\n","    df_copy.rename(columns = {'GPM':'MediaGPM','GPV':'MediaGPV','CM':'MediaCM','CV':'MediaCV','CAAM':'MediaCAAM','CAAV':'MediaCAAV'}, inplace = True)\n","    df_medias_rodada_treino = df_copy[min:max+1]\n","    novos_dfs_rodadas.append(df_medias_rodada_treino)\n","    '''print(Fore.GREEN+Style.BRIGHT+\"RODADA: %d\" % rodada+Style.RESET_ALL)\n","    display(df_medias_rodada)\n","    print('\\n')\n","    print('\\n')'''\n","    rodada += 1\n","  df_medias_temporada = df_temp.append(novos_dfs_rodadas,ignore_index=True)\n","  return df_medias_temporada\n","\n","''' Função que armazena todos os dataframes com médias de cada temporada em uma lista '''\n","def gera_lista_medias_temporadas(dfs):  \n","  # Lista com os dfs das temproadas com as medias, rodada a rodada.\n","  dfs_medias_temporadas = []\n","  for df in dfs:\n","    temp_df = gera_df_medias_temporada(df)\n","    dfs_medias_temporadas.append(temp_df)\n","  return dfs_medias_temporadas\n","\n","'''Função que retorna os dataframes concatenados dos treinos base,armazenados em uma lista'''\n","def gera_lista_temporadas_treino_base(lista_medias_temporadas):\n","  # Concatena temporadas treino base:\n","  lista = lista_medias_temporadas\n","  i = 0\n","  base_01 = 0\n","  base_02 = 1\n","  base_03 = 2\n","  datasets_de_treinos_iniciais = []\n","  while i < 7:\n","    df_01 = lista[base_01]\n","    df_02 = lista[base_02]\n","    df_03 = lista[base_03]\n","    temp_df = pd.concat([df_01,df_02,df_03],ignore_index=True)\n","    datasets_de_treinos_iniciais.append(temp_df)\n","    ##### Iteração:\n","    base_01 += 1\n","    base_02 += 1\n","    base_03 += 1\n","    i += 1\n","  return datasets_de_treinos_iniciais\n","\n","'''Método que retorna um dataframe com médias da rodada a ser preditada'''\n","def gera_novo_df_com_medias_da_rodada_teste(rodada,dataFrame):\n","  min = (rodada - 5) * 10\n","  max = min + 10\n","  return dataFrame.iloc[min:max,0:11]\n","\n","'''Método que junta o dataframe com médias da rodada ao treino com médias'''\n","def concatena_rodada_teste_ao_treino(df_treino,df_medias_rodada_teste):\n","  size = len(df_treino)\n","  temp_df = pd.concat([df_treino,df_medias_rodada_teste], ignore_index=True)\n","  size += 10\n","  temp_df.index = range(size)\n","  return temp_df\n","\n","'''Méotodo que prepara o dataframe para ser inserido como dataset X na Rede Neural; serve para treino e para teste'''\n","def cria_X(dataFrame):\n","  temp_df = dataFrame.drop(columns = ['Mandante', 'Visitante', 'Resultado','oddLayH'])\n","  return temp_df\n","\n","'''Méotodo que prepara o dataframe para ser inserido como dataset Y na Rede neural; serve para treino e para teste'''\n","def cria_Y(dataFrame):\n","  temp_df = dataFrame.drop(columns = ['Mandante', 'Visitante','MediaGPM','MediaGPV','MediaCM','MediaCV','MediaCAAM','MediaCAAV','oddH','oddLayH'])\n","  return temp_df\n","  \n","'''Méotodo que prepara o df da rodada para ser predições, com apenas as odds e o resultado da partida'''\n","def cria_base_predicao_da_rodada(dataFrame):\n","  temp_df = dataFrame.drop(columns = ['Mandante', 'Visitante','MediaGPM','MediaGPV','MediaCM','MediaCV','MediaCAAM','MediaCAAV'])\n","  return temp_df"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-WOPifgXS4q"},"source":["**2.EXECUÇÃO**"]},{"cell_type":"code","metadata":{"id":"nzSJ3-UoWY7u","executionInfo":{"status":"ok","timestamp":1624033584196,"user_tz":180,"elapsed":42278,"user":{"displayName":"Caio Medeiros","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQG435jXGXnbmJ1O_I3S2jlpLhSSV6mfsEHit2=s64","userId":"03125895587331329917"}}},"source":["path = '/content/drive/MyDrive/Iniciação Científica - UAEst - Prof. Alexsandro_2019.2/Pesquisa_Caio Medeiros/Projeto 2020_2021_Deep Learning/Tabelas - Premier League/Tabelas Premier League/'\n","dfs = load_data(path)\n","\n","lista_medias_temporadas = gera_lista_medias_temporadas(dfs) # Lista de todas as temporadas com médias\n","lista_treinos_base = gera_lista_temporadas_treino_base(lista_medias_temporadas) # Lista de todos os treinos base (7 elementos)"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJMEKQgr-wQB"},"source":["**ENTRADA DE PARÂMETROS**"]},{"cell_type":"code","metadata":{"id":"4ZpfV3VY-1-r","executionInfo":{"status":"ok","timestamp":1624033668511,"user_tz":180,"elapsed":291,"user":{"displayName":"Caio Medeiros","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQG435jXGXnbmJ1O_I3S2jlpLhSSV6mfsEHit2=s64","userId":"03125895587331329917"}}},"source":["neuronios = 64 # 64 | 128 | 128\n","camadas = 10   # 10 |  4  | 8"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TdyelJe_Emp"},"source":["**EXECUÇÃO DA REDE**"]},{"cell_type":"code","metadata":{"id":"MTOlTIcL6MjO","executionInfo":{"status":"ok","timestamp":1624034361653,"user_tz":180,"elapsed":689031,"user":{"displayName":"Caio Medeiros","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQG435jXGXnbmJ1O_I3S2jlpLhSSV6mfsEHit2=s64","userId":"03125895587331329917"}}},"source":["model = tf.keras.models.Sequential()\n","#model.add(tf.keras.layers.Dense(neuronios, input_dim = 7, kernel_initializer = 'uniform', activation = 'relu'))\n","for i in range(camadas):\n","  if i == 0:\n","    model.add(tf.keras.layers.Dense(neuronios, input_dim = 7, kernel_initializer = 'uniform', activation = 'relu'))\n","    if camadas == 1:\n","      break\n","  else:\n","    model.add(tf.keras.layers.Dense(neuronios, activation = 'relu'))\n","\n","\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n","model.compile(optimizer = opt, loss='binary_crossentropy', metrics=['accuracy'])\n","mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose = 0, save_best_only=True)\n","\n","temporada = 2013\n","teste = 3\n","treino_base = 0\n","stats_geral = []\n","analise_exp_temporadas = []\n","while temporada < 2020 and treino_base < 7:\n","\n","  if temporada > 2013:\n","    saved_model = load_model('best_model.h5')\n","    saved_model.compile(optimizer = opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    mc = ModelCheckpoint('best_model.h5', monitor = 'val_loss', mode='min', verbose = 0, save_best_only=True)\n","\n","  temp_df = lista_treinos_base[treino_base] # vai do 0 ao 6 (7 treinos ao total)\n","  rodada = 5\n","  ganhos = 0\n","  perdas = 0\n","  tp = 0\n","  fp = 0\n","  fn = 0\n","  tn = 0\n","  tp_rodada = 0\n","  fp_rodada = 0\n","  fn_rodada = 0\n","  tn_rodada = 0\n","  analise_exp = []\n","  while rodada <= 38:\n","\n","    '''if rodada > 5:\n","      saved_model = load_model('best_model.h5')\n","      saved_model.compile(tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n","      mc = ModelCheckpoint('best_model.h5', monitor = 'val_loss', mode='min', verbose = 0, save_best_only=True)'''\n","    \n","    # Montando os datasets de Treino X e Y (começará a partir dar 4ª round e terá médias em vez de valores base, com as 50 primeira linhas depois das 30 iniciais):\n","    data_x_train = cria_X(temp_df)\n","    data_y_train = cria_Y(temp_df)        \n","    x_train = tf.convert_to_tensor(data_x_train, dtype = tf.int32)  \n","    y_train = tf.convert_to_tensor(data_y_train, dtype = tf.int32)\n","    # Montando os datasets de teste X e Y:\n","    df_rodada_teste = gera_novo_df_com_medias_da_rodada_teste(rodada,lista_medias_temporadas[teste])\n","    data_x_test = cria_X(df_rodada_teste)\n","    data_y_test = cria_Y(df_rodada_teste)\n","    x_test = tf.convert_to_tensor(data_x_test, dtype = tf.float32)  \n","    y_test = tf.convert_to_tensor(data_y_test, dtype = tf.float32)  \n","\n","    # Treinando o modelo sem/com parada antecipada e/ou decaimento:\n","    lr_sched = step_decay_schedule(initial_lr = 0.0001, decay_factor = 0.25, step_size = 10)\n","    \n","    es = EarlyStopping(monitor='val_loss', mode='min', verbose = 0, patience = 17)\n","    model.fit(x_train, y_train, validation_split = 0.2, epochs = 100,callbacks =[es,mc], verbose = 0)  # Analisar os parametros possiveis. Verificar Sobreajuste e Subajuste\n","    # Analisando as predições e o Lucro/prejuízo:\n","    df_base_predicoes = cria_base_predicao_da_rodada(df_rodada_teste)\n","    rna_predicoes = model.predict(x_test)\n","    \n","    for i in range(10):\n","      oddH = df_base_predicoes.iloc[i][1]\n","      if rna_predicoes[i] >= 0.6:\n","        if df_base_predicoes.iloc[i][0] == 1:\n","          tp += 1\n","          tp_rodada += 1\n","          ganhos += 100*(oddH - 1) \n","        else:\n","          fp += 1\n","          fp_rodada += 1\n","          perdas += 100\n","\n","    # Últimas etapas:\n","    temp_df = concatena_rodada_teste_ao_treino(temp_df,df_rodada_teste)\n","    rodada += 1\n","\n","    analise_exp.append([tp_rodada,fp_rodada])\n","    tp_rodada = 0\n","    fp_rodada = 0\n","\n","  teste += 1\n","  treino_base += 1\n","  total_de_entradas = tp+fp\n","  acc = acuracia(tp,0,total_de_entradas)\n","  prec = precisao(tp,fp)\n","  sensi = sensibilidade(tp,fp)\n","  f1_s = f1_score(tp,fp,0)\n","\n","  # GErando Palnilha de analise exploratoria da temporada:\n","  colunas = ['AcertoVMandante','ErroVMandante']\n","  index_rodadas = []\n","  for i in range(5,39):\n","    index_rodadas.append(str(i))\n","  rows = np.array(analise_exp)\n","  df_02 = pd.DataFrame(rows,columns = colunas,index = index_rodadas)\n","  writer = '/content/drive/MyDrive/Iniciação Científica - UAEst - Prof. Alexsandro_2019.2/Pesquisa_Caio Medeiros/Projeto 2020_2021_Deep Learning/Resultados/Resultados_Melhores com estratégias/Só No mandante/Análise Exploratória/Temporada_'+str(temporada)+'_'+str(camadas)+'_Analise_Exploratoria_com_'+str(neuronios)+'_neuronios.xlsx'\n","  df_02.to_excel(writer)\n","\n","  stats_geral.append([tp,fp,0,0,acc,prec,sensi,f1_s,ganhos,perdas,ganhos-perdas])\n","  \n","  '''print(\"A Acurácia do algoritmo na temporada %d foi: %.2f%%\" % (temporada,acc*100))\n","  print(\"A precisão do algoritmo na temporada %d foi: %.2f\" % (temporada,prec))\n","  print(\"A Sensibilidade do algoritmo na temporada %d foi: %.2f\" % (temporada,sensi))\n","  print(\"O f1_score do algoritmo na temporada %d foi: %.2f\" % (temporada,f1_s))\n","  print(\"O Ganho total da temporada %d foi: R$ %.2f\" % (temporada,ganhos))\n","  print(\"A Perda total da temporada %d foi: R$ %.2f\" % (temporada,perdas))\n","  if ganhos > perdas:\n","    print(Fore.GREEN+Style.BRIGHT+\"O lucro da temporada %d foi: R$ %.2f\" % (temporada,ganhos-perdas)+Style.RESET_ALL)\n","  else:\n","    print(Fore.RED+Style.BRIGHT+\"O prejuízo da temporada %d foi: R$ %.2f\" % (temporada,ganhos-perdas)+Style.RESET_ALL)\n","  print(\"\\n\")'''\n","\n","  temporada += 1\n","\n","temps = ['2013','2014','2015','2016','2017','2018','2019']\n","cols = ['AcertoVMandante','ErroVMandante','ErroDMandante','AcertoDMandante','Acurácia','Precisão','Sensibilidade','F1_Score','Ganhos','Perdas','L/P',]\n","rows = np.array(np.round(stats_geral,2))\n","df = pd.DataFrame(rows,columns=cols,index = temps)\n","writer = '/content/drive/MyDrive/Iniciação Científica - UAEst - Prof. Alexsandro_2019.2/Pesquisa_Caio Medeiros/Projeto 2020_2021_Deep Learning/Resultados/Resultados_Melhores com estratégias/Só No mandante/Resultado Geral/resultado_'+str(camadas)+'_com_'+str(neuronios)+'_neuronios.xlsx'\n","df.to_excel(writer)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"vc0EamJAsaue"},"source":[""],"execution_count":null,"outputs":[]}]}